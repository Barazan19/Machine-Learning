# Decision Trees (Classification)

This repository provides a **step-by-step, intuitive explanation of how Decision Tree Classifiers work**, starting from scratch and progressing to Scikit-Learn implementations. The goal is to **build intuition through manual calculations**, visualizations, and real-world datasets — following the teaching style of Josh Starmer's StatQuest series.

---

## 📚 Table of Contents

- [📁 File Overview](#-file-overview)
- [📊 Dataset](#-dataset)
- [🧠 Concepts Covered](#-concepts-covered)
- [📌 Goals](#-goals)
- [📽️ Inspired by](#-inspired-by)
- [✍️ Author](#️-author)

---

## 📁 File Overview

### 1. [`Manual_Decision_Tree_Classifier.ipynb`](https://github.com/Barazan19/Machine-Learning/blob/main/Decision%20Trees%20(Classification)/Manual_Decision_Tree_Classifier.ipynb)
This notebook builds a **Decision Tree Classifier from scratch** using the `insurance_claim_dataset.csv`. It walks through each core concept:
- Calculating **Gini Impurity**
- Evaluating **information gain**
- Manually selecting the **best split**
- **Recursive splitting** until pure leaf nodes are found
- Includes **step-by-step visualizations** of the process

📌 No libraries like Scikit-learn are used in this notebook — it's all manually implemented for learning.

---

### 2. [`Decision_Tree_Classifier_Scikit_Learn.ipynb`](https://github.com/Barazan19/Machine-Learning/blob/main/Decision%20Trees%20(Classification)/Decision_Tree_Classifier_Scikit_Learn.ipynb)
This notebook applies **Scikit-Learn's `DecisionTreeClassifier`** on the same dataset to:
- Compare against the manual version
- Analyze model complexity (tree depth, number of leaves)
- Visualize the decision tree using `graphviz`
- Evaluate model performance via **accuracy score**, **confusion matrix**, etc.

We also include a simple **GridSearchCV** example to tune `max_depth` and explore how it affects the model.

---

### 3. [`Compare_Manual_vs_Sklearn_Tree.ipynb`](https://github.com/Barazan19/Machine-Learning/blob/main/Decision%20Trees%20(Classification)/Compare_Manual_vs_Sklearn_Tree.ipynb)
Here, we directly compare the **structure** and **predictions** of the manually built tree vs. the one generated by Scikit-Learn. Key observations include:
- Tree complexity differences
- Matching or diverging splits
- Accuracy comparisons

---

## 📊 Dataset

We use the `insurance_claim_dataset.csv`, which is a **binary classification dataset** for predicting whether a claim will be made or not, based on features like:
- `age`
- `bmi`
- `children`
- `smoker`, etc.

---

## 🧠 Concepts Covered

- Gini Impurity & Information Gain
- Recursive Splitting
- Stopping Conditions
- Decision Tree Visualization
- Model Evaluation (Confusion Matrix, Accuracy)
- Comparison between Manual and Library-based Models
- Introduction to Hyperparameter Tuning (Grid Search)

---

## 📌 Goals

✅ Build intuition from the ground up  
✅ Visualize and demystify the decision-making process  
✅ Understand how tree depth and impurity affect performance  
✅ Lay a strong foundation before exploring **Bagging**, **Boosting**, and **Random Forests**

---

## 📽️ Inspired by

This notebook series is heavily inspired by the **StatQuest with Josh Starmer** YouTube series. Special thanks to his clear, visual explanations which serve as the foundation for these exercises.

---

## ✍️ Author

**Made by [Barazan19](https://github.com/Barazan19), to learn and visualize machine learning intuitively from first principles.**
